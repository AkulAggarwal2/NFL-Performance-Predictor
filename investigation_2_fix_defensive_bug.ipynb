{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigation 2: Fix Defensive Stats Bug\n",
    "\n",
    "**Problem**: Defensive stats showed -3.9% impact (worst individual feature)\n",
    "\n",
    "**Suspected Bug**:\n",
    "```python\n",
    "features['defensive_ppg'] = defensive_plays['epa'].sum() * 6 / weeks\n",
    "```\n",
    "\n",
    "**Why multiply by 6?** This artificially inflates defensive points allowed by 6x!\n",
    "\n",
    "**Expected**: Fixing this should flip defensive stats from -3.9% to +1-2% (helpful)\n",
    "\n",
    "**Approach**:\n",
    "1. Copy ablation study code\n",
    "2. Fix defensive formula (remove √ó 6)\n",
    "3. Re-run test #6 (Baseline + Defensive Stats)\n",
    "4. Compare results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nfl_data_py as nfl\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, brier_score_loss, roc_auc_score\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data (Reuse from Ablation Study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NFL data...\n",
      "(This takes 5-10 minutes on first run)\n",
      "\n",
      "2015 done.\n",
      "2016 done.\n",
      "2017 done.\n",
      "2018 done.\n",
      "2019 done.\n",
      "2020 done.\n",
      "2021 done.\n",
      "2022 done.\n",
      "2023 done.\n",
      "2024 done.\n",
      "Downcasting floats.\n",
      "‚úÖ Loaded 483,605 plays\n",
      "Downcasting floats.\n",
      "‚úÖ Loaded 54,479 player-game records\n",
      "‚úÖ Loaded 2,743 scheduled games\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading NFL data...\")\n",
    "print(\"(This takes 5-10 minutes on first run)\\n\")\n",
    "\n",
    "START_YEAR = 2015\n",
    "END_YEAR = 2024\n",
    "TEST_YEAR = 2024\n",
    "\n",
    "pbp_data = nfl.import_pbp_data(years=range(START_YEAR, END_YEAR + 1))\n",
    "print(f\"‚úÖ Loaded {len(pbp_data):,} plays\")\n",
    "\n",
    "weekly_data = nfl.import_weekly_data(years=range(START_YEAR, END_YEAR + 1))\n",
    "print(f\"‚úÖ Loaded {len(weekly_data):,} player-game records\")\n",
    "\n",
    "schedule_data = nfl.import_schedules(years=range(START_YEAR, END_YEAR + 1))\n",
    "print(f\"‚úÖ Loaded {len(schedule_data):,} scheduled games\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Feature Engineering Functions\n",
    "\n",
    "### ORIGINAL (Buggy) Defensive Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Buggy defensive function defined (for comparison)\n"
     ]
    }
   ],
   "source": [
    "def add_defensive_features_BUGGY(features, pbp_data, team, season, week):\n",
    "    \"\"\"\n",
    "    BUGGY VERSION: Multiplies EPA by 6 (artificially inflates defensive points)\n",
    "    \"\"\"\n",
    "    defensive_plays = pbp_data[\n",
    "        (pbp_data['defteam'] == team) &\n",
    "        (pbp_data['season'] == season) &\n",
    "        (pbp_data['week'] < week)\n",
    "    ]\n",
    "    \n",
    "    if len(defensive_plays) > 0:\n",
    "        weeks = defensive_plays['week'].nunique()\n",
    "        features['defensive_ypg'] = defensive_plays['yards_gained'].sum() / weeks if weeks > 0 else 0\n",
    "        # BUG: Why multiply by 6?\n",
    "        features['defensive_ppg'] = defensive_plays['epa'].sum() * 6 / weeks if weeks > 0 else 0\n",
    "    else:\n",
    "        features['defensive_ypg'] = 0\n",
    "        features['defensive_ppg'] = 0\n",
    "    \n",
    "    return features\n",
    "\n",
    "print(\"‚úÖ Buggy defensive function defined (for comparison)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIXED Defensive Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fixed defensive function defined\n",
      "\n",
      "üîç KEY CHANGE: defensive_ppg √ó 6 ‚Üí defensive_epa_pg (no multiplier)\n"
     ]
    }
   ],
   "source": [
    "def add_defensive_features_FIXED(features, pbp_data, team, season, week):\n",
    "    \"\"\"\n",
    "    FIXED VERSION: Removed √ó 6 multiplication\n",
    "    \"\"\"\n",
    "    defensive_plays = pbp_data[\n",
    "        (pbp_data['defteam'] == team) &\n",
    "        (pbp_data['season'] == season) &\n",
    "        (pbp_data['week'] < week)\n",
    "    ]\n",
    "    \n",
    "    if len(defensive_plays) > 0:\n",
    "        weeks = defensive_plays['week'].nunique()\n",
    "        features['defensive_ypg'] = defensive_plays['yards_gained'].sum() / weeks if weeks > 0 else 0\n",
    "        # FIXED: Removed √ó 6\n",
    "        features['defensive_epa_pg'] = defensive_plays['epa'].sum() / weeks if weeks > 0 else 0\n",
    "    else:\n",
    "        features['defensive_ypg'] = 0\n",
    "        features['defensive_epa_pg'] = 0\n",
    "    \n",
    "    return features\n",
    "\n",
    "print(\"‚úÖ Fixed defensive function defined\")\n",
    "print(\"\\nüîç KEY CHANGE: defensive_ppg √ó 6 ‚Üí defensive_epa_pg (no multiplier)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Legacy Features (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Legacy feature function defined\n"
     ]
    }
   ],
   "source": [
    "def create_team_features_legacy(weekly_data, team, season, week):\n",
    "    \"\"\"\n",
    "    Legacy Week 9 feature engineering (baseline).\n",
    "    \"\"\"\n",
    "    team_stats = weekly_data[\n",
    "        (weekly_data['recent_team'] == team) &\n",
    "        (weekly_data['season'] == season) &\n",
    "        (weekly_data['week'] < week)\n",
    "    ]\n",
    "    \n",
    "    if team_stats.empty:\n",
    "        return {}\n",
    "    \n",
    "    features = {\n",
    "        'passing_ypg': team_stats['passing_yards'].sum() / len(team_stats['week'].unique()) if not team_stats.empty else 0,\n",
    "        'rushing_ypg': team_stats['rushing_yards'].sum() / len(team_stats['week'].unique()) if not team_stats.empty else 0,\n",
    "        'total_ypg': (team_stats['passing_yards'].sum() + team_stats['rushing_yards'].sum()) / len(team_stats['week'].unique()) if not team_stats.empty else 0,\n",
    "        'points_pg': team_stats.groupby('week')['fantasy_points'].sum().mean() if not team_stats.empty else 0,\n",
    "        'passing_tds_pg': team_stats['passing_tds'].sum() / len(team_stats['week'].unique()) if not team_stats.empty else 0,\n",
    "        'turnovers_pg': (team_stats['interceptions'].sum() + team_stats.get('fumbles_lost', pd.Series([0])).sum()) / len(team_stats['week'].unique()) if not team_stats.empty else 0,\n",
    "    }\n",
    "    \n",
    "    return features\n",
    "\n",
    "print(\"‚úÖ Legacy feature function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Predictor Class (No Configuration Needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DefensiveTestPredictor class created\n"
     ]
    }
   ],
   "source": [
    "class DefensiveTestPredictor:\n",
    "    \"\"\"\n",
    "    Simplified predictor for testing defensive stats fix.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, use_fixed_defensive=True):\n",
    "        self.use_fixed = use_fixed_defensive\n",
    "        self.model = None\n",
    "    \n",
    "    def build_features(self, pbp_data, weekly_data, schedule_data, season, week):\n",
    "        games = schedule_data[\n",
    "            (schedule_data['season'] == season) &\n",
    "            (schedule_data['week'] == week)\n",
    "        ]\n",
    "        \n",
    "        X_list = []\n",
    "        y_list = []\n",
    "        \n",
    "        for _, game in games.iterrows():\n",
    "            home_team = game['home_team']\n",
    "            away_team = game['away_team']\n",
    "            \n",
    "            # Legacy features\n",
    "            home_features = create_team_features_legacy(weekly_data, home_team, season, week)\n",
    "            away_features = create_team_features_legacy(weekly_data, away_team, season, week)\n",
    "            \n",
    "            if not home_features or not away_features:\n",
    "                continue\n",
    "            \n",
    "            # Add defensive features (fixed or buggy)\n",
    "            if self.use_fixed:\n",
    "                home_features = add_defensive_features_FIXED(home_features, pbp_data, home_team, season, week)\n",
    "                away_features = add_defensive_features_FIXED(away_features, pbp_data, away_team, season, week)\n",
    "            else:\n",
    "                home_features = add_defensive_features_BUGGY(home_features, pbp_data, home_team, season, week)\n",
    "                away_features = add_defensive_features_BUGGY(away_features, pbp_data, away_team, season, week)\n",
    "            \n",
    "            # Combine\n",
    "            combined = {}\n",
    "            for key in home_features.keys():\n",
    "                combined[f'home_{key}'] = home_features[key]\n",
    "                combined[f'away_{key}'] = away_features.get(key, 0)\n",
    "            \n",
    "            combined['scoring_advantage'] = home_features.get('points_pg', 0) - away_features.get('points_pg', 0)\n",
    "            combined['turnover_advantage'] = away_features.get('turnovers_pg', 0) - home_features.get('turnovers_pg', 0)\n",
    "            combined['is_playoff'] = 1 if week >= 18 else 0\n",
    "            combined['season'] = season\n",
    "            \n",
    "            X_list.append(combined)\n",
    "            \n",
    "            if pd.notna(game.get('home_score')) and pd.notna(game.get('away_score')):\n",
    "                y_list.append(1 if game['home_score'] > game['away_score'] else 0)\n",
    "            else:\n",
    "                y_list.append(None)\n",
    "        \n",
    "        X = pd.DataFrame(X_list)\n",
    "        y = pd.Series(y_list)\n",
    "        \n",
    "        valid_mask = y.notna()\n",
    "        return X[valid_mask], y[valid_mask]\n",
    "    \n",
    "    def train(self, X_train, y_train):\n",
    "        # Simple 3-model ensemble, depth=5\n",
    "        rf = RandomForestClassifier(n_estimators=200, max_depth=5, random_state=42)\n",
    "        lr = LogisticRegression(C=1.0, max_iter=1000, random_state=42)\n",
    "        xgb_model = xgb.XGBClassifier(max_depth=5, learning_rate=0.1, n_estimators=200, random_state=42)\n",
    "        \n",
    "        voting = VotingClassifier([('rf', rf), ('lr', lr), ('xgb', xgb_model)], voting='soft')\n",
    "        self.model = CalibratedClassifierCV(voting, method='isotonic', cv=3)\n",
    "        self.model.fit(X_train, y_train)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        y_proba = self.model.predict_proba(X_test)[:, 1]\n",
    "        return y_pred, y_proba\n",
    "\n",
    "print(\"‚úÖ DefensiveTestPredictor class created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Buggy vs Fixed on Sample Weeks\n",
    "\n",
    "Quick test on a few weeks to see the difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing BUGGY vs FIXED defensive stats on Week 15-17 of 2024...\n",
      "\n",
      "\n",
      "============================================================\n",
      "Testing BUGGY version\n",
      "============================================================\n",
      "  Week 15: 81.2% (13/16)\n",
      "  Week 16: 81.2% (13/16)\n",
      "  Week 17: 68.8% (11/16)\n",
      "\n",
      "  Overall (Weeks 15-17): 77.1% (37/48)\n",
      "\n",
      "============================================================\n",
      "Testing FIXED version\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m year \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2015\u001b[39m, \u001b[38;5;241m2024\u001b[39m):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m week \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m19\u001b[39m):\n\u001b[0;32m---> 21\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m predictor\u001b[38;5;241m.\u001b[39mbuild_features(pbp_data, weekly_data, schedule_data, year, week)\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(X) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     23\u001b[0m             X_train_list\u001b[38;5;241m.\u001b[39mappend(X)\n",
      "Cell \u001b[0;32mIn[6], line 32\u001b[0m, in \u001b[0;36mDefensiveTestPredictor.build_features\u001b[0;34m(self, pbp_data, weekly_data, schedule_data, season, week)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Add defensive features (fixed or buggy)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_fixed:\n\u001b[0;32m---> 32\u001b[0m     home_features \u001b[38;5;241m=\u001b[39m add_defensive_features_FIXED(home_features, pbp_data, home_team, season, week)\n\u001b[1;32m     33\u001b[0m     away_features \u001b[38;5;241m=\u001b[39m add_defensive_features_FIXED(away_features, pbp_data, away_team, season, week)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m, in \u001b[0;36madd_defensive_features_FIXED\u001b[0;34m(features, pbp_data, team, season, week)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_defensive_features_FIXED\u001b[39m(features, pbp_data, team, season, week):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    FIXED VERSION: Removed √ó 6 multiplication\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     defensive_plays \u001b[38;5;241m=\u001b[39m pbp_data[\n\u001b[0;32m----> 6\u001b[0m         (pbp_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefteam\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m team) \u001b[38;5;241m&\u001b[39m\n\u001b[1;32m      7\u001b[0m         (pbp_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseason\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m season) \u001b[38;5;241m&\u001b[39m\n\u001b[1;32m      8\u001b[0m         (pbp_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweek\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m week)\n\u001b[1;32m      9\u001b[0m     ]\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(defensive_plays) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     12\u001b[0m         weeks \u001b[38;5;241m=\u001b[39m defensive_plays[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweek\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnunique()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/ops/common.py:72\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     70\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m method(\u001b[38;5;28mself\u001b[39m, other)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/arraylike.py:42\u001b[0m, in \u001b[0;36mOpsMixin.__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__eq__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cmp_method(other, operator\u001b[38;5;241m.\u001b[39meq)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/series.py:6243\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6240\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   6242\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 6243\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mcomparison_op(lvalues, rvalues, op)\n\u001b[1;32m   6245\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(res_values, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/ops/array_ops.py:287\u001b[0m, in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invalid_comparison(lvalues, rvalues, op)\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_object_dtype(lvalues\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rvalues, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 287\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m comp_method_OBJECT_ARRAY(op, lvalues, rvalues)\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m _na_arithmetic_op(lvalues, rvalues, op, is_cmp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/ops/array_ops.py:75\u001b[0m, in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m     73\u001b[0m     result \u001b[38;5;241m=\u001b[39m libops\u001b[38;5;241m.\u001b[39mvec_compare(x\u001b[38;5;241m.\u001b[39mravel(), y\u001b[38;5;241m.\u001b[39mravel(), op)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 75\u001b[0m     result \u001b[38;5;241m=\u001b[39m libops\u001b[38;5;241m.\u001b[39mscalar_compare(x\u001b[38;5;241m.\u001b[39mravel(), y, op)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Testing BUGGY vs FIXED defensive stats on Week 15-17 of 2024...\\n\")\n",
    "\n",
    "for use_fixed in [False, True]:\n",
    "    version = \"FIXED\" if use_fixed else \"BUGGY\"\n",
    "    predictor = DefensiveTestPredictor(use_fixed_defensive=use_fixed)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing {version} version\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    all_correct = 0\n",
    "    all_total = 0\n",
    "    \n",
    "    for test_week in [15, 16, 17]:\n",
    "        # Train on all prior data\n",
    "        X_train_list = []\n",
    "        y_train_list = []\n",
    "        \n",
    "        for year in range(2015, 2024):\n",
    "            for week in range(1, 19):\n",
    "                X, y = predictor.build_features(pbp_data, weekly_data, schedule_data, year, week)\n",
    "                if len(X) > 0:\n",
    "                    X_train_list.append(X)\n",
    "                    y_train_list.append(y)\n",
    "        \n",
    "        for week in range(1, test_week):\n",
    "            X, y = predictor.build_features(pbp_data, weekly_data, schedule_data, 2024, week)\n",
    "            if len(X) > 0:\n",
    "                X_train_list.append(X)\n",
    "                y_train_list.append(y)\n",
    "        \n",
    "        X_train = pd.concat(X_train_list, ignore_index=True)\n",
    "        y_train = pd.concat(y_train_list, ignore_index=True)\n",
    "        \n",
    "        # Test\n",
    "        X_test, y_test = predictor.build_features(pbp_data, weekly_data, schedule_data, 2024, test_week)\n",
    "        \n",
    "        if len(X_test) > 0:\n",
    "            predictor.train(X_train, y_train)\n",
    "            y_pred, y_proba = predictor.predict(X_test)\n",
    "            \n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            print(f\"  Week {test_week}: {acc:.1%} ({(y_pred == y_test).sum()}/{len(y_test)})\")\n",
    "            \n",
    "            all_correct += (y_pred == y_test).sum()\n",
    "            all_total += len(y_test)\n",
    "    \n",
    "    overall_acc = all_correct / all_total if all_total > 0 else 0\n",
    "    print(f\"\\n  Overall (Weeks 15-17): {overall_acc:.1%} ({all_correct}/{all_total})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full 2024 Test: Rerun Ablation Study Test #6\n",
    "\n",
    "Complete walk-forward validation on all of 2024:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_test(use_fixed_defensive=True):\n",
    "    \"\"\"\n",
    "    Replicate Test #6 from ablation study with fixed defensive stats.\n",
    "    \"\"\"\n",
    "    version = \"FIXED\" if use_fixed_defensive else \"BUGGY (Original)\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Full 2024 Test: Baseline + Defensive Stats ({version})\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    predictor = DefensiveTestPredictor(use_fixed_defensive=use_fixed_defensive)\n",
    "    \n",
    "    all_preds = []\n",
    "    all_actuals = []\n",
    "    all_probas = []\n",
    "    \n",
    "    for test_week in range(1, 19):\n",
    "        print(f\"  Testing week {test_week}...\", end=\" \")\n",
    "        \n",
    "        X_train_list = []\n",
    "        y_train_list = []\n",
    "        \n",
    "        # Historical data\n",
    "        for train_year in range(2015, 2024):\n",
    "            for train_week in range(1, 19):\n",
    "                X_week, y_week = predictor.build_features(pbp_data, weekly_data, schedule_data, train_year, train_week)\n",
    "                if len(X_week) > 0:\n",
    "                    X_train_list.append(X_week)\n",
    "                    y_train_list.append(y_week)\n",
    "        \n",
    "        # 2024 up to test_week-1\n",
    "        for train_week in range(1, test_week):\n",
    "            X_week, y_week = predictor.build_features(pbp_data, weekly_data, schedule_data, 2024, train_week)\n",
    "            if len(X_week) > 0:\n",
    "                X_train_list.append(X_week)\n",
    "                y_train_list.append(y_week)\n",
    "        \n",
    "        if len(X_train_list) == 0:\n",
    "            print(\"No training data\")\n",
    "            continue\n",
    "        \n",
    "        X_train = pd.concat(X_train_list, ignore_index=True)\n",
    "        y_train = pd.concat(y_train_list, ignore_index=True)\n",
    "        \n",
    "        X_test, y_test = predictor.build_features(pbp_data, weekly_data, schedule_data, 2024, test_week)\n",
    "        \n",
    "        if len(X_test) == 0:\n",
    "            print(\"No test data\")\n",
    "            continue\n",
    "        \n",
    "        predictor.train(X_train, y_train)\n",
    "        y_pred, y_proba = predictor.predict(X_test)\n",
    "        \n",
    "        all_preds.extend(y_pred)\n",
    "        all_actuals.extend(y_test)\n",
    "        all_probas.extend(y_proba)\n",
    "        \n",
    "        week_acc = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Acc: {week_acc:.1%}\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_actuals, all_preds)\n",
    "    brier = brier_score_loss(all_actuals, all_probas)\n",
    "    auc = roc_auc_score(all_actuals, all_probas)\n",
    "    \n",
    "    # High-confidence\n",
    "    hc_mask = np.array([(p > 0.70 or p < 0.30) for p in all_probas])\n",
    "    if hc_mask.sum() > 0:\n",
    "        hc_preds = np.array(all_preds)[hc_mask]\n",
    "        hc_actuals = np.array(all_actuals)[hc_mask]\n",
    "        hc_accuracy = accuracy_score(hc_actuals, hc_preds)\n",
    "    else:\n",
    "        hc_accuracy = None\n",
    "    \n",
    "    print(f\"\\nüìä RESULTS ({version}):\")\n",
    "    print(f\"  Accuracy: {accuracy:.1%}\")\n",
    "    print(f\"  HC Accuracy: {hc_accuracy:.1%}\" if hc_accuracy else \"  HC Accuracy: N/A\")\n",
    "    print(f\"  Brier Score: {brier:.3f}\")\n",
    "    print(f\"  AUC-ROC: {auc:.3f}\")\n",
    "    print(f\"  Games: {len(all_actuals)}\")\n",
    "    \n",
    "    return accuracy, hc_accuracy, brier\n",
    "\n",
    "# Run both versions\n",
    "print(\"\\nRunning full 2024 test with BOTH versions...\")\n",
    "print(\"(This takes ~15-20 minutes)\\n\")\n",
    "\n",
    "buggy_acc, buggy_hc, buggy_brier = run_full_test(use_fixed_defensive=False)\n",
    "fixed_acc, fixed_hc, fixed_brier = run_full_test(use_fixed_defensive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARISON: BUGGY vs FIXED Defensive Stats\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n{'Metric':<20} {'BUGGY (√ó 6)':<20} {'FIXED':<20} {'Delta':<15}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "delta_acc = (fixed_acc - buggy_acc) * 100\n",
    "delta_hc = (fixed_hc - buggy_hc) * 100 if buggy_hc and fixed_hc else 0\n",
    "delta_brier = fixed_brier - buggy_brier\n",
    "\n",
    "print(f\"{'Accuracy':<20} {buggy_acc:>18.1%} {fixed_acc:>18.1%} {delta_acc:>+13.1f} pts\")\n",
    "if buggy_hc and fixed_hc:\n",
    "    print(f\"{'HC Accuracy':<20} {buggy_hc:>18.1%} {fixed_hc:>18.1%} {delta_hc:>+13.1f} pts\")\n",
    "print(f\"{'Brier Score':<20} {buggy_brier:>18.3f} {fixed_brier:>18.3f} {delta_brier:>+13.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if delta_acc > 0.5:\n",
    "    print(f\"\\n‚úÖ CONFIRMED: Bug fixed! Defensive stats improved by {delta_acc:.1f} percentage points\")\n",
    "    print(f\"   This validates the hypothesis that √ó 6 was causing the -3.9% harm.\")\n",
    "elif delta_acc < -0.5:\n",
    "    print(f\"\\n‚ùå UNEXPECTED: Fixed version performed WORSE by {abs(delta_acc):.1f} percentage points\")\n",
    "    print(f\"   The √ó 6 may have been intentional or there's another issue.\")\n",
    "else:\n",
    "    print(f\"\\n‚ö™ NEUTRAL: Minimal difference ({delta_acc:.1f} pts)\")\n",
    "    print(f\"   The √ó 6 bug may not be the primary cause of defensive stats issues.\")\n",
    "\n",
    "print(\"\\nVs Baseline (66.8% from ablation study):\")\n",
    "baseline = 0.668\n",
    "buggy_vs_baseline = (buggy_acc - baseline) * 100\n",
    "fixed_vs_baseline = (fixed_acc - baseline) * 100\n",
    "\n",
    "print(f\"  Buggy defensive: {buggy_vs_baseline:+.1f} pts vs baseline\")\n",
    "print(f\"  Fixed defensive: {fixed_vs_baseline:+.1f} pts vs baseline\")\n",
    "\n",
    "if fixed_vs_baseline > 1.0:\n",
    "    print(f\"\\n‚úÖ CONCLUSION: Fixed defensive stats are HELPFUL (+{fixed_vs_baseline:.1f}%)\")\n",
    "    print(f\"   Recommendation: KEEP defensive stats with fixed formula\")\n",
    "elif fixed_vs_baseline < -1.0:\n",
    "    print(f\"\\n‚ùå CONCLUSION: Even fixed, defensive stats are HARMFUL ({fixed_vs_baseline:+.1f}%)\")\n",
    "    print(f\"   Recommendation: REMOVE defensive stats entirely\")\n",
    "else:\n",
    "    print(f\"\\n‚ö™ CONCLUSION: Defensive stats are NEUTRAL ({fixed_vs_baseline:+.1f}%)\")\n",
    "    print(f\"   Recommendation: Skip defensive stats, not worth complexity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_summary = {\n",
    "    'buggy_accuracy': buggy_acc,\n",
    "    'fixed_accuracy': fixed_acc,\n",
    "    'delta': delta_acc / 100,\n",
    "    'buggy_vs_baseline': buggy_vs_baseline / 100,\n",
    "    'fixed_vs_baseline': fixed_vs_baseline / 100\n",
    "}\n",
    "\n",
    "pd.DataFrame([results_summary]).to_csv('investigation_2_results.csv', index=False)\n",
    "print(\"\\n‚úÖ Results saved to: investigation_2_results.csv\")\n",
    "\n",
    "with open('investigation_2_findings.md', 'w') as f:\n",
    "    f.write(\"# Investigation 2: Defensive Stats Bug Fix Results\\n\\n\")\n",
    "    f.write(f\"**Date**: {pd.Timestamp.now().strftime('%Y-%m-%d')}\\n\\n\")\n",
    "    \n",
    "    f.write(\"## Bug Description\\n\\n\")\n",
    "    f.write(\"```python\\n\")\n",
    "    f.write(\"# BUGGY:\\n\")\n",
    "    f.write(\"features['defensive_ppg'] = defensive_plays['epa'].sum() * 6 / weeks\\n\\n\")\n",
    "    f.write(\"# FIXED:\\n\")\n",
    "    f.write(\"features['defensive_epa_pg'] = defensive_plays['epa'].sum() / weeks\\n\")\n",
    "    f.write(\"```\\n\\n\")\n",
    "    \n",
    "    f.write(\"## Results\\n\\n\")\n",
    "    f.write(f\"- **Buggy version**: {buggy_acc:.1%} accuracy\\n\")\n",
    "    f.write(f\"- **Fixed version**: {fixed_acc:.1%} accuracy\\n\")\n",
    "    f.write(f\"- **Delta**: {delta_acc:+.1f} percentage points\\n\\n\")\n",
    "    \n",
    "    f.write(\"## Recommendation\\n\\n\")\n",
    "    if fixed_vs_baseline > 1.0:\n",
    "        f.write(f\"‚úÖ **KEEP** defensive stats with fixed formula ({fixed_vs_baseline:+.1f}% vs baseline)\\n\")\n",
    "    elif fixed_vs_baseline < -1.0:\n",
    "        f.write(f\"‚ùå **REMOVE** defensive stats entirely ({fixed_vs_baseline:+.1f}% vs baseline)\\n\")\n",
    "    else:\n",
    "        f.write(f\"‚ö™ **SKIP** defensive stats, not worth complexity ({fixed_vs_baseline:+.1f}% vs baseline)\\n\")\n",
    "\n",
    "print(\"‚úÖ Findings saved to: investigation_2_findings.md\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INVESTIGATION 2 COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
