{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Failure Pattern Analysis: Week 10+ Performance Degradation\n",
    "\n",
    "**Objective**: Analyze existing Week 1-14 prediction results to identify systematic failure patterns.\n",
    "\n",
    "**Key Questions**:\n",
    "1. Why did high-confidence picks fail catastrophically (33.3% in Weeks 13-14)?\n",
    "2. Are there specific game types that the model struggles with?\n",
    "3. Is there home team bias?\n",
    "4. How does calibration differ between legacy (1-9) vs enhanced (10-14)?\n",
    "\n",
    "**Data Source**: Existing CSV prediction files in Week*/week*_predictions.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nfl_data_py as nfl\n",
    "from pathlib import Path\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✅ Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load All Prediction Results (Weeks 1-14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predictions from CSV files\n",
    "base_path = Path('/Users/akulaggarwal/Desktop/NFL Performance Prediction')\n",
    "all_predictions = []\n",
    "\n",
    "for week in range(1, 15):\n",
    "    csv_path = base_path / f'Week{week}' / f'week{week}_predictions.csv'\n",
    "    \n",
    "    if csv_path.exists():\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df['week'] = week\n",
    "        df['model_type'] = 'legacy' if week <= 9 else 'enhanced'\n",
    "        all_predictions.append(df)\n",
    "        print(f\"✅ Loaded Week {week}: {len(df)} games\")\n",
    "    else:\n",
    "        print(f\"❌ Week {week} not found\")\n",
    "\n",
    "if all_predictions:\n",
    "    predictions_df = pd.concat(all_predictions, ignore_index=True)\n",
    "    print(f\"\\n✅ Total predictions loaded: {len(predictions_df)} games across {predictions_df['week'].nunique()} weeks\")\n",
    "else:\n",
    "    print(\"\\n❌ No predictions found. Please ensure CSV files exist.\")\n",
    "    predictions_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Actual Results (2025 Season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not predictions_df.empty:\n",
    "    print(\"Fetching 2025 NFL schedule and results...\")\n",
    "    \n",
    "    # Load 2025 schedule\n",
    "    schedule_2025 = nfl.import_schedules([2025])\n",
    "    \n",
    "    if schedule_2025.empty:\n",
    "        print(\"⚠️ 2025 data not available, trying 2024...\")\n",
    "        schedule_2025 = nfl.import_schedules([2024])\n",
    "    \n",
    "    print(f\"✅ Loaded {len(schedule_2025)} games from schedule\")\n",
    "    \n",
    "    # Match predictions to actual results\n",
    "    predictions_df['actual_winner'] = None\n",
    "    predictions_df['home_score'] = None\n",
    "    predictions_df['away_score'] = None\n",
    "    predictions_df['correct'] = None\n",
    "    \n",
    "    for idx, row in predictions_df.iterrows():\n",
    "        week = row['week']\n",
    "        away_team = row['away_team']\n",
    "        home_team = row['home_team']\n",
    "        \n",
    "        # Find game in schedule\n",
    "        game = schedule_2025[\n",
    "            (schedule_2025['week'] == week) &\n",
    "            (schedule_2025['away_team'].str.upper() == away_team.upper()) &\n",
    "            (schedule_2025['home_team'].str.upper() == home_team.upper())\n",
    "        ]\n",
    "        \n",
    "        if not game.empty and pd.notna(game.iloc[0]['home_score']):\n",
    "            home_score = game.iloc[0]['home_score']\n",
    "            away_score = game.iloc[0]['away_score']\n",
    "            actual_winner = home_team if home_score > away_score else away_team\n",
    "            \n",
    "            predictions_df.at[idx, 'actual_winner'] = actual_winner\n",
    "            predictions_df.at[idx, 'home_score'] = home_score\n",
    "            predictions_df.at[idx, 'away_score'] = away_score\n",
    "            predictions_df.at[idx, 'correct'] = (row['predicted_winner'] == actual_winner)\n",
    "    \n",
    "    # Filter to completed games only\n",
    "    completed_mask = predictions_df['correct'].notna()\n",
    "    predictions_df = predictions_df[completed_mask].copy()\n",
    "    \n",
    "    print(f\"\\n✅ Matched {len(predictions_df)} completed games with actual results\")\n",
    "    print(f\"   Legacy (Weeks 1-9): {len(predictions_df[predictions_df['model_type']=='legacy'])} games\")\n",
    "    print(f\"   Enhanced (Weeks 10-14): {len(predictions_df[predictions_df['model_type']=='enhanced'])} games\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 1: Overall Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not predictions_df.empty:\n",
    "    print(\"=\"*70)\n",
    "    print(\"PERFORMANCE COMPARISON: LEGACY vs ENHANCED\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for model_type in ['legacy', 'enhanced']:\n",
    "        subset = predictions_df[predictions_df['model_type'] == model_type]\n",
    "        \n",
    "        accuracy = subset['correct'].mean()\n",
    "        n_games = len(subset)\n",
    "        n_correct = subset['correct'].sum()\n",
    "        \n",
    "        # High-confidence accuracy (>65%)\n",
    "        hc_mask = subset['confidence'] > 0.65\n",
    "        hc_accuracy = subset[hc_mask]['correct'].mean() if hc_mask.sum() > 0 else None\n",
    "        hc_games = hc_mask.sum()\n",
    "        \n",
    "        weeks = sorted(subset['week'].unique())\n",
    "        \n",
    "        print(f\"\\n{model_type.upper()} MODEL (Weeks {weeks[0]}-{weeks[-1]}):\")\n",
    "        print(f\"  Overall Accuracy: {accuracy:.1%} ({n_correct}/{n_games})\")\n",
    "        print(f\"  High-Conf Picks (>65%): {hc_accuracy:.1%} ({subset[hc_mask]['correct'].sum()}/{hc_games})\" if hc_accuracy else \"  High-Conf Picks: N/A\")\n",
    "        print(f\"  Avg Confidence: {subset['confidence'].mean():.1%}\")\n",
    "        print(f\"  Std Dev (week-to-week): {subset.groupby('week')['correct'].mean().std():.1%}\")\n",
    "    \n",
    "    # Calculate delta\n",
    "    legacy_acc = predictions_df[predictions_df['model_type']=='legacy']['correct'].mean()\n",
    "    enhanced_acc = predictions_df[predictions_df['model_type']=='enhanced']['correct'].mean()\n",
    "    delta = enhanced_acc - legacy_acc\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"DELTA: Enhanced vs Legacy = {delta:+.1%} ({(delta*100):+.1f} percentage points)\")\n",
    "    print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 2: Calibration Analysis (Confidence vs Actual Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not predictions_df.empty:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CALIBRATION ANALYSIS: Predicted Confidence vs Actual Accuracy\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Create confidence bins\n",
    "    bins = [0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 1.0]\n",
    "    bin_labels = ['50-55%', '55-60%', '60-65%', '65-70%', '70-75%', '75%+']\n",
    "    \n",
    "    predictions_df['conf_bin'] = pd.cut(predictions_df['confidence'], bins=bins, labels=bin_labels, include_lowest=True)\n",
    "    \n",
    "    for model_type in ['legacy', 'enhanced']:\n",
    "        subset = predictions_df[predictions_df['model_type'] == model_type]\n",
    "        \n",
    "        print(f\"\\n{model_type.upper()} MODEL:\")\n",
    "        print(f\"{'Confidence Bin':<15} {'N Games':<10} {'Actual Acc':<15} {'Calibration Gap':<20}\")\n",
    "        print(\"-\"*70)\n",
    "        \n",
    "        for bin_label in bin_labels:\n",
    "            bin_data = subset[subset['conf_bin'] == bin_label]\n",
    "            \n",
    "            if len(bin_data) > 0:\n",
    "                actual_acc = bin_data['correct'].mean()\n",
    "                expected_conf = bin_data['confidence'].mean()\n",
    "                gap = actual_acc - expected_conf\n",
    "                \n",
    "                gap_str = f\"{gap:+.1%} {'(overconfident)' if gap < 0 else '(underconfident)' if gap > 0 else '(calibrated)'}\" \n",
    "                \n",
    "                print(f\"{bin_label:<15} {len(bin_data):<10} {actual_acc:<14.1%} {gap_str}\")\n",
    "            else:\n",
    "                print(f\"{bin_label:<15} {0:<10} {'N/A':<14} {'N/A'}\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    for idx, model_type in enumerate(['legacy', 'enhanced']):\n",
    "        subset = predictions_df[predictions_df['model_type'] == model_type]\n",
    "        \n",
    "        calibration = subset.groupby('conf_bin').agg({\n",
    "            'correct': 'mean',\n",
    "            'confidence': 'mean'\n",
    "        }).reset_index()\n",
    "        \n",
    "        ax = axes[idx]\n",
    "        x = np.arange(len(calibration))\n",
    "        width = 0.35\n",
    "        \n",
    "        ax.bar(x - width/2, calibration['confidence'], width, label='Predicted Confidence', alpha=0.7)\n",
    "        ax.bar(x + width/2, calibration['correct'], width, label='Actual Accuracy', alpha=0.7)\n",
    "        \n",
    "        ax.set_xlabel('Confidence Bin')\n",
    "        ax.set_ylabel('Percentage')\n",
    "        ax.set_title(f'{model_type.title()} Model Calibration')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(calibration['conf_bin'], rotation=45)\n",
    "        ax.legend()\n",
    "        ax.set_ylim([0, 1])\n",
    "        ax.axhline(y=0.5, color='r', linestyle='--', alpha=0.3, label='Baseline')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('calibration_analysis.png', dpi=150)\n",
    "    print(\"\\n✅ Calibration chart saved to: calibration_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 3: Home Team Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not predictions_df.empty:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"HOME TEAM BIAS ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    predictions_df['predicted_home_win'] = predictions_df['predicted_winner'] == predictions_df['home_team']\n",
    "    predictions_df['actual_home_win'] = predictions_df['actual_winner'] == predictions_df['home_team']\n",
    "    \n",
    "    for model_type in ['legacy', 'enhanced']:\n",
    "        subset = predictions_df[predictions_df['model_type'] == model_type]\n",
    "        \n",
    "        # How often does model predict home win?\n",
    "        pred_home_rate = subset['predicted_home_win'].mean()\n",
    "        \n",
    "        # What's actual home win rate?\n",
    "        actual_home_rate = subset['actual_home_win'].mean()\n",
    "        \n",
    "        # Accuracy on home wins vs away wins\n",
    "        home_win_preds = subset[subset['predicted_home_win']]\n",
    "        away_win_preds = subset[~subset['predicted_home_win']]\n",
    "        \n",
    "        home_acc = home_win_preds['correct'].mean()\n",
    "        away_acc = away_win_preds['correct'].mean()\n",
    "        \n",
    "        # Recall: Of actual home wins, how many did we predict?\n",
    "        actual_home_wins = subset[subset['actual_home_win']]\n",
    "        home_recall = (actual_home_wins['predicted_home_win']).mean()\n",
    "        \n",
    "        # Recall: Of actual away wins, how many did we predict?\n",
    "        actual_away_wins = subset[~subset['actual_home_win']]\n",
    "        away_recall = (~actual_away_wins['predicted_home_win']).mean()\n",
    "        \n",
    "        print(f\"\\n{model_type.upper()} MODEL:\")\n",
    "        print(f\"  Predicted home win rate: {pred_home_rate:.1%}\")\n",
    "        print(f\"  Actual home win rate: {actual_home_rate:.1%}\")\n",
    "        print(f\"  BIAS: {pred_home_rate - actual_home_rate:+.1%} {'(over-predicting home wins)' if pred_home_rate > actual_home_rate else '(under-predicting home wins)'}\")\n",
    "        print(f\"\\n  When predicting home win: {home_acc:.1%} accuracy ({len(home_win_preds)} games)\")\n",
    "        print(f\"  When predicting away win: {away_acc:.1%} accuracy ({len(away_win_preds)} games)\")\n",
    "        print(f\"\\n  Home win recall: {home_recall:.1%} (caught {home_recall:.1%} of actual home wins)\")\n",
    "        print(f\"  Away win recall: {away_recall:.1%} (caught {away_recall:.1%} of actual away wins)\")\n",
    "        \n",
    "        if home_recall > 0.70 and away_recall < 0.40:\n",
    "            print(f\"  ⚠️ SEVERE HOME BIAS DETECTED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 4: Division Game Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not predictions_df.empty:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"DIVISION GAME ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Define divisions (simplified - this would need full team mapping)\n",
    "    afc_east = ['BUF', 'MIA', 'NE', 'NYJ']\n",
    "    afc_north = ['BAL', 'CIN', 'CLE', 'PIT']\n",
    "    afc_south = ['HOU', 'IND', 'JAX', 'TEN']\n",
    "    afc_west = ['DEN', 'KC', 'LAC', 'LV']\n",
    "    nfc_east = ['DAL', 'NYG', 'PHI', 'WAS']\n",
    "    nfc_north = ['CHI', 'DET', 'GB', 'MIN']\n",
    "    nfc_south = ['ATL', 'CAR', 'NO', 'TB']\n",
    "    nfc_west = ['ARI', 'LA', 'SF', 'SEA']\n",
    "    \n",
    "    divisions = [afc_east, afc_north, afc_south, afc_west, nfc_east, nfc_north, nfc_south, nfc_west]\n",
    "    \n",
    "    def is_division_game(away, home):\n",
    "        for div in divisions:\n",
    "            if away in div and home in div:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    predictions_df['is_division_game'] = predictions_df.apply(\n",
    "        lambda row: is_division_game(row['away_team'], row['home_team']), axis=1\n",
    "    )\n",
    "    \n",
    "    for model_type in ['legacy', 'enhanced']:\n",
    "        subset = predictions_df[predictions_df['model_type'] == model_type]\n",
    "        \n",
    "        div_games = subset[subset['is_division_game']]\n",
    "        non_div_games = subset[~subset['is_division_game']]\n",
    "        \n",
    "        div_acc = div_games['correct'].mean() if len(div_games) > 0 else None\n",
    "        non_div_acc = non_div_games['correct'].mean() if len(non_div_games) > 0 else None\n",
    "        \n",
    "        print(f\"\\n{model_type.upper()} MODEL:\")\n",
    "        print(f\"  Division games: {div_acc:.1%} accuracy ({len(div_games)} games)\" if div_acc else \"  Division games: N/A\")\n",
    "        print(f\"  Non-division games: {non_div_acc:.1%} accuracy ({len(non_div_games)} games)\" if non_div_acc else \"  Non-division games: N/A\")\n",
    "        \n",
    "        if div_acc and non_div_acc:\n",
    "            delta = div_acc - non_div_acc\n",
    "            print(f\"  DELTA: {delta:+.1%} {'(division games harder)' if delta < 0 else '(division games easier)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 5: Worst Performing Games (Enhanced Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not predictions_df.empty:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"HIGH-CONFIDENCE FAILURES (Enhanced Model, Weeks 10-14)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    enhanced = predictions_df[predictions_df['model_type'] == 'enhanced']\n",
    "    \n",
    "    # High-confidence failures (>65% confidence but wrong)\n",
    "    hc_failures = enhanced[(enhanced['confidence'] > 0.65) & (~enhanced['correct'])]\n",
    "    \n",
    "    print(f\"\\nFound {len(hc_failures)} high-confidence failures:\")\n",
    "    print(f\"\\n{'Week':<6} {'Matchup':<20} {'Predicted':<10} {'Actual':<10} {'Confidence':<12} {'Score':<12} {'Is Div?'}\")\n",
    "    print(\"-\"*90)\n",
    "    \n",
    "    for _, game in hc_failures.sort_values('confidence', ascending=False).iterrows():\n",
    "        is_div = 'YES' if game.get('is_division_game', False) else 'NO'\n",
    "        score = f\"{int(game['away_score'])}-{int(game['home_score'])}\"\n",
    "        \n",
    "        print(f\"{game['week']:<6} {game['matchup']:<20} {game['predicted_winner']:<10} {game['actual_winner']:<10} {game['confidence']:<11.1%} {score:<12} {is_div}\")\n",
    "    \n",
    "    # Patterns\n",
    "    if len(hc_failures) > 0:\n",
    "        div_failures = hc_failures['is_division_game'].sum() if 'is_division_game' in hc_failures.columns else 0\n",
    "        print(f\"\\nPATTERNS:\")\n",
    "        print(f\"  Division games: {div_failures}/{len(hc_failures)} ({div_failures/len(hc_failures):.1%})\")\n",
    "        print(f\"  Avg confidence: {hc_failures['confidence'].mean():.1%}\")\n",
    "        print(f\"  Weeks affected: {sorted(hc_failures['week'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not predictions_df.empty:\n",
    "    # Save annotated predictions\n",
    "    predictions_df.to_csv('failure_pattern_analysis_results.csv', index=False)\n",
    "    print(\"\\n✅ Full analysis saved to: failure_pattern_analysis_results.csv\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FAILURE PATTERN ANALYSIS COMPLETE\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"1. Review calibration analysis - are high-confidence picks overconfident?\")\n",
    "    print(\"2. Check home team bias - is model systematically favoring home teams?\")\n",
    "    print(\"3. Examine division game performance - NFL parity makes these harder\")\n",
    "    print(\"4. Run debug_enhanced_model.ipynb for full ablation study (2-4 hours)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
